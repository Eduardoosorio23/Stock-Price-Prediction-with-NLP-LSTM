{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H5jVRoGJlhaL",
    "outputId": "03395095-74f6-4203-c68e-a73498c9c258"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('/content/gdrive')\n",
    "# root_path = '/D/Flatiron/Mod_5/Capstone'  #change dir to your project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJNvfW4Xmj5l",
    "outputId": "8fb6cc5e-463f-4102-bbd8-645d4a5ce640"
   },
   "outputs": [],
   "source": [
    "# pip install pandas_datareader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip uninstall ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "03lDV6k10OPk",
    "outputId": "45733583-b4b1-440a-c8d3-70db2edfb62b"
   },
   "outputs": [],
   "source": [
    "#import Libraries\n",
    "import math\n",
    "import pandas_datareader as web\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import config\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from  nltk import FreqDist\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.collocations import *\n",
    "# nltk.download('wordnet')\n",
    "from nltk import word_tokenize, FreqDist\n",
    "\n",
    "from spacy import displacy\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import twint\n",
    "import pandas as pd\n",
    "\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()\n",
    "\n",
    "from dateutil import rrule\n",
    "import datetime as datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f612b874be8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcurrentDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'TSLA'\u001b[0m\u001b[1;31m#ticker symbol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'wallstreetbets'\u001b[0m \u001b[1;31m#Subreddit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "currentDate = datetime.datetime.now()\n",
    "q = 'TSLA'#ticker symbol\n",
    "s = 'wallstreetbets' #Subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "currentDate.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dates = []\n",
    "\n",
    "for i in range(12):\n",
    "    Dates.append((currentDate - relativedelta(weeks=i)).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "hundredDaysLater = now - timedelta(days=300)\n",
    "\n",
    "for dt in rrule.rrule(rrule.DAILY, dtstart=now, until=hundredDaysLater):\n",
    "    print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPHaaXtSua9h"
   },
   "outputs": [],
   "source": [
    "r = praw.Reddit(\n",
    "    client_id=config.reddit['client_id'],\n",
    "    client_secret=config.reddit['client_secret'],\n",
    "    username=config.reddit['username'],\n",
    "    password=config.reddit['password'],\n",
    "    user_agent='test'\n",
    ")\n",
    "api = PushshiftAPI(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a function to streamline webscraping\n",
    "def populate(currentDate, q, subreddit, limit):\n",
    "#Reddit Posts    \n",
    "    Dates = []\n",
    "    gen = []\n",
    "\n",
    "    for i in range(24):#Number of months\n",
    "        Dates.append((currentDate - relativedelta(months=i)).date())\n",
    "    \n",
    "    for i in range(len(Dates)-1):\n",
    "\n",
    "        \n",
    "        gen.extend(list(api.search_submissions(after=Dates[i+1],\n",
    "                            before=Dates[i],\n",
    "                            q=q,\n",
    "                            subreddit='wallstreetbets',\n",
    "                            filter=['created', 'title'],\n",
    "                            check_for_async=False,\n",
    "                            limit=limit)))\n",
    "    \n",
    "#Turning gen list into a pandas dataframe    \n",
    "    j = []\n",
    "\n",
    "    for i in gen:\n",
    "        j.append([i.title, \n",
    "#               i.author, \n",
    "#                   i.subreddit, \n",
    "#               i.score, \n",
    "                datetime.datetime.fromtimestamp(\n",
    "                int(round(i.created))\n",
    "                ).strftime('%Y-%m-%d')])#Returns the time the post was created in regular time instead of UNIX time])\n",
    "        \n",
    "    red_posts = pd.DataFrame(j,columns=['post', \n",
    "#                                 'author', \n",
    "#                                 'subreddit', \n",
    "#                                 'score', \n",
    "                                'date'])\n",
    "    \n",
    "    \n",
    "#Tweets dataframe\n",
    "    \n",
    "    c = twint.Config()\n",
    "\n",
    "    c.Search =q #search must contain \"\"\n",
    "    c.Min_likes = 1000 #min number of likes\n",
    "    c.Until = str(currentDate.date()) #return tweets that were published before this date\n",
    "    c.Since = str((currentDate - relativedelta(months=24)).date()) #return tweets published after this day(Months)\n",
    "    c.Count = True\n",
    "    c.Limit = 15000 # Limits the tweets to this number\n",
    "    # c.Format = \"Tweet id: {id} | Date: {date} | Username: {username} | Tweet: {tweet} | Mention: {mention}\"\n",
    "    c.Store_csv = True\n",
    "    c.Output = 'Gamestop.csv'\n",
    "    \n",
    "    twint.run.Search(c)\n",
    "    \n",
    "    tweets = pd.read_csv('Gamestop.csv')\n",
    "    tweets = tweets.rename(columns={\"tweet\": \"post\"})\n",
    "\n",
    "    \n",
    "    tweets = pd.concat([tweets['date'], tweets['post']], axis=1)\n",
    "    \n",
    "    \n",
    "#combining reddit post and tweets    \n",
    "    posts = pd.concat([red_posts, tweets])\n",
    "    \n",
    "    return posts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = populate(currentDate, q, s, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "R3RwBHVzztxM",
    "outputId": "e2f3fab0-4591-4a7a-f0a9-39783620d0f0"
   },
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kxz7fGslzuYl",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "posts = posts.drop_duplicates(subset=['post'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP Pipeline\n",
    "nlp.component_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['spacy'] = posts.post.progress_apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts['sentiment'] = posts.spacy.progress_apply(lambda x: (x._.polarity))\n",
    "posts['subjectivity'] = posts.spacy.progress_apply(lambda x: (x._.subjectivity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(posts, x='sentiment', y='subjectivity',\n",
    "                hover_data=['post'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created two seperate dataframes for positive and negative posts\n",
    "pos_list = posts[posts['sentiment'] > 0.0]\n",
    "neg_list = posts[posts['sentiment'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_posts_concat = []\n",
    "neg_posts_concat = []\n",
    "\n",
    "for post in pos_list['post']:\n",
    "    pos_posts_concat += post\n",
    "for post in neg_list['post']:\n",
    "    neg_posts_concat += post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created stop words list\n",
    "stopwords_list = stopwords.words('english') + list(string.punctuation)\n",
    "stopwords_list += [\"''\", '\"\"', '...', '``', 'link', 'tesla', 'gamestop', \"'\", \"'s\", 'â€™', 'retard', '[', ']', 'http', 'tsla',\n",
    "                  'gme','tsla', 'http', 'https', \"n't\", 'fuck', 'stock', 'retards', 'amc', 'buy', 'people']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for tokenized and lemmantized the post\n",
    "def process_post(posts):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(posts)\n",
    "    stopwords_removed = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stopwords_list]\n",
    "    return stopwords_removed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fed boths postive and negative posts\n",
    "pos_words = list(map(process_post, neg_list['post']))\n",
    "neg_words = list(map(process_post, pos_list['post']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words_list = []\n",
    "for post in pos_words:\n",
    "    for word in post:\n",
    "        pos_words_list.append(word)\n",
    "        \n",
    "neg_words_list = []\n",
    "for post in neg_words:\n",
    "    for word in post:\n",
    "        neg_words_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_post_freqdist = FreqDist(pos_words_list)\n",
    "pos_post_freqdist = pd.DataFrame(pos_post_freqdist.most_common(10), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing most common positive posts\n",
    "fig1 = px.bar(pos_post_freqdist, y='count', x='word', \n",
    "              hover_data=['count'],\n",
    "              color='count',\n",
    "              title=' Most frequent positive')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_post_freqdist = FreqDist(neg_words_list)\n",
    "neg_post_freqdist = pd.DataFrame(neg_post_freqdist.most_common(10), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing most negative posts\n",
    "fig1 = px.bar(neg_post_freqdist, y='count', x='word', \n",
    "              hover_data=['count'],\n",
    "              color='count',\n",
    "              title=' Most frequent Negative')\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping the posts by day and then returning average values for sentiment and subjectivity\n",
    "final_posts = posts.groupby(['date']).mean().sort_values(by='date', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_posts.index.names = ['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "_RA_-iX12M7y",
    "outputId": "c905441f-5ed1-463f-b690-19e91dfcd605",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Get the stock quote\n",
    "tick = web.DataReader(q, data_source='yahoo', start=str((currentDate - relativedelta(months=12)).date()), end=str(currentDate.date()))\n",
    "tick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= tick.join(final_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaaLsFOP3MW7",
    "outputId": "3361e360-587b-4c43-9f16-c0148174fd04",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filled NaN values with the average of the columns\n",
    "df = df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'] = np.append(df['Close'].iloc[1:].values, [np.nan])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "Lz8iOY453fQl",
    "outputId": "f0c198b9-29f4-4706-97e8-34d23ddc139d"
   },
   "outputs": [],
   "source": [
    "#Visualize The data\n",
    "fig,ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "ax.set_title('Closing Price and Sentiment History')\n",
    "ax.plot(df['Close'], color='red')\n",
    "ax.set_xlabel('Date', fontsize=18)\n",
    "ax.set_ylabel('Closing Price USD ($)',fontsize=18)\n",
    "\n",
    "ax2=ax.twinx()\n",
    "ax2.plot(df['sentiment'], color='Blue', marker=\"o\")\n",
    "ax2.set_xlabel('Date', fontsize=18)\n",
    "ax2.set_ylabel('Sentiment',fontsize=18)\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Capstone",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
